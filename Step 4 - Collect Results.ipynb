{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.hp_functions import get_dset_hp # Helper functions to load default hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931be305",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {'office-home': ['AC', 'AP', 'AR', 'CA', 'CP', 'CR', 'PA', 'PC', 'PR', 'RA', 'RC', 'RP'],\n",
    "         'visda': ['SR', 'RS']\n",
    "        }\n",
    "methods = ['s. only', 'pada', 'safn', 'ba3us', 'ar', 'jumbot', 'mpot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(logs_folder, dataset, methods, hp):\n",
    "    table = {}\n",
    "    _, domains = get_dset_hp(dataset, '')\n",
    "    seeds = [2020,2021,2022]\n",
    "    net = 'ResNet50'\n",
    "    count = 0\n",
    "    for hp_metric in ['oracle', '1shot_acc', '50random_acc', '100random_acc', 's_acc', 'ent', 'dev_svm', 'snd']:\n",
    "        table[hp_metric] = {}\n",
    "        for sel_metric in ['final', 'oracle', '1shot_acc', '50random_acc', '100random_acc', 's_acc', 'ent', 'dev_svm', 'snd']:\n",
    "            table[hp_metric][sel_metric] = np.zeros((len(methods),len(domains)*(len(domains)-1), len(seeds)))\n",
    "        for ix, method in enumerate(methods):\n",
    "            jx = 0\n",
    "            for source_domain in domains:\n",
    "                for target_domain in domains:\n",
    "                    if source_domain != target_domain:\n",
    "                        task = source_domain[0].upper() + target_domain[0].upper()\n",
    "                        for kx, seed in enumerate(seeds):\n",
    "                            output_dir = os.path.join(logs_folder, method, net, dataset, task)\n",
    "                            for key in hp[method][hp_metric]:\n",
    "                                output_dir = os.path.join(output_dir, f'{key}_{hp[method][hp_metric][key]}')\n",
    "                            path_results = os.path.join(output_dir, f\"seed_{seed}\", 'run_0', 'results.npy')\n",
    "                            if os.path.exists(path_results):\n",
    "                                results = np.load(path_results, allow_pickle=True).item()\n",
    "                                t_acc = np.array(results['t_acc'])\n",
    "                                table[hp_metric]['final'][ix,jx,kx] = t_acc[-1]\n",
    "                                table[hp_metric]['oracle'][ix,jx,kx] = t_acc.max()\n",
    "                                for temp_metric in ['ent', 'dev_svm']:\n",
    "                                    table[hp_metric][temp_metric][ix,jx,kx] = t_acc[np.array(results[temp_metric]).argmin()]\n",
    "                                for temp_metric in ['snd', '1shot_acc', '100random_acc', 's_acc']:\n",
    "                                    table[hp_metric][temp_metric][ix,jx,kx] = t_acc[np.array(results[temp_metric]).argmax()]\n",
    "                                if hp_metric in ['50random_acc']:\n",
    "                                    for temp_metric in ['50random_acc']:\n",
    "                                        if temp_metric in results.keys():\n",
    "                                            table[hp_metric][temp_metric][ix,jx,kx] = t_acc[np.array(results[temp_metric]).argmax()]\n",
    "#                                         else:\n",
    "#                                             count += 1\n",
    "#                                             print(f'Missing {hp_metric} on {task} for {method} with seed {seed}')                                                \n",
    "                        jx += 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26fe49c",
   "metadata": {},
   "source": [
    "## Gather tables for the main results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c3a48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tables = {}\n",
    "logs_folder = 'logs_hp_chosen'\n",
    "methods = ['source_only_plus', 'pada', 'safn', 'ba3us', 'ar', 'jumbot', 'mpot']\n",
    "for dataset in ['office-home', 'visda']:\n",
    "    hp = np.load('results/hp_chosen.npy', allow_pickle=True).item()\n",
    "    hp = hp[dataset]\n",
    "    tables[dataset] = get_tables(logs_folder, dataset, methods, hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1fbb5",
   "metadata": {},
   "source": [
    "## Gather tables with feature normalization in bottleneck layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696d7d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tables_radius = {}\n",
    "logs_folder = 'logs_hp_chosen_radius'\n",
    "methods = ['ar']\n",
    "for dataset in ['office-home']:\n",
    "    hp = np.load('results/hp_chosen_radius.npy', allow_pickle=True).item()\n",
    "    hp = hp[dataset]\n",
    "    tables_radius[dataset] = get_tables(logs_folder, dataset, methods, hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578ce24",
   "metadata": {},
   "source": [
    "## Gather tables with nonlinear bottleneck layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b791799",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_nonlinear = {}\n",
    "logs_folder = 'logs_hp_chosen_nonlinear'\n",
    "methods = ['safn']\n",
    "for dataset in ['office-home']:\n",
    "    hp = np.load('results/hp_chosen_nonlinear.npy', allow_pickle=True).item()\n",
    "    hp = hp[dataset]\n",
    "    tables_nonlinear[dataset] = get_tables(logs_folder, dataset, methods, hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f01df",
   "metadata": {},
   "source": [
    "Counting number of models trained for main results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = np.load('results/hp_chosen.npy', allow_pickle=True).item()\n",
    "hp_metrics = ['oracle', '1shot_acc', '100random_acc', 's_acc', 'ent', 'dev_svm', 'snd']\n",
    "for dataset in ['office-home', 'visda']:\n",
    "    total = 0\n",
    "    for method in ['pada', 'safn', 'ar', 'ba3us', 'jumbot', 'mpot']:\n",
    "        hp_values = hp[dataset][method]['oracle'].keys()\n",
    "        table = np.zeros((len(hp_metrics), len(hp_values)))\n",
    "        for ix, metric in enumerate(hp_metrics):\n",
    "            for jx, param in enumerate(hp_values):\n",
    "                table[ix,jx] = hp[dataset][method][metric][param]\n",
    "        count = np.unique(table,axis=0).shape[0]*3*len(tasks[dataset])\n",
    "        print(f'For {method} we trained {count} models.')\n",
    "        total += count\n",
    "    print(f'For {dataset} we trained a total {total} models.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b0601",
   "metadata": {},
   "source": [
    "## Reported results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef2d4c",
   "metadata": {},
   "source": [
    "We gather from the original papers the reported results for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reported = {'office-home': {}, 'visda': {}}\n",
    "\n",
    "reported['office-home']['s. only'] = [46.33, 67.51, 75.87, 59.14, 59.94, 62.73, 58.22, 41.79, 74.88, 67.40, 48.18, 74.17]\n",
    "reported['office-home']['ar_linear'] = [62.13, 79.22, 89.12, 73.92, 75.57, 84.37, 78.42, 61.91, 87.85, 82.19, 65.37, 85.27] #Avg  77.11\n",
    "reported['office-home']['ar'] = [67.40, 85.32, 90.00, 77.32, 70.59, 85.15, 78.97, 64.78, 89.51, 80.44, 66.21, 86.44] #Avg  78.29\n",
    "reported['office-home']['ba3us'] = [60.62, 83.16, 88.39, 71.75, 72.79, 83.40, 75.45, 61.59, 86.53, 79.25, 62.80, 86.05] #Avg 75.98\n",
    "#BA3US stds [0.45, 0.12, 0.19, 0.19, 1.10, 0.59, 0.19, 0.37, 0.22, 0.65, 0.51, 0.26]\n",
    "reported['office-home']['safn'] = [58.93, 76.25, 81.42, 70.43, 72.97, 77.78, 72.36, 55.34, 80.4, 75.81, 60.42, 79.92] #Avg 71.83\n",
    "#SAFN stds [0.50, 0.33, 0.27, 0.46, 1.39, 0.52, 0.31, 0.46, 0.78, 0.37, 0.83, 0.20]\n",
    "reported['office-home']['pada'] = [51.95, 67, 78.74, 52.16, 53.78, 59.03, 52.61, 43.22, 78.79, 73.73, 56.6, 77.09] #Avg  62.06\n",
    "reported['office-home']['jumbot'] = [62.7, 77.5, 84.4, 76.0, 73.3, 80.5, 74.7, 60.8, 85.1, 80.2, 66.5, 83.9] #Avg  75.5\n",
    "reported['office-home']['mpot'] = [64.60, 80.62, 87.17, 76.43, 77.61, 83.58, 77.07, 63.74, 87.63, 81.42, 68.50, 87.38] #Avg  77.98\n",
    "\n",
    "reported['visda']['s. only'] = [45.26, 64.28] # Avg 54.77\n",
    "reported['visda']['ba3us'] = [69.86, 67.56] # Avg 68.71\n",
    "reported['visda']['pada'] = [53.53, 76.50] # Avg 65.01\n",
    "reported['visda']['safn'] = [67.65, '-']\n",
    "reported['visda']['ar_linear'] = [85.30, 74.82] # Avg  80.09\n",
    "reported['visda']['ar'] = [88.75, 78.52] # Avg  83.62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a2ad7",
   "metadata": {},
   "source": [
    "## Code to generate Table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['s. only', 'pada', 'safn', 'ba3us', 'ar', 'jumbot', 'mpot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'office-home'\n",
    "table = ['\\\\begin{table}[t!]\\\\centering']\n",
    "table.append('\\\\resizebox{\\\\textwidth}{!}{')\n",
    "temp = '\\\\begin{tabular}{c'\n",
    "for i in range(len(tasks)+1):\n",
    "    temp += '@{\\hskip 3pt}|c'\n",
    "temp += '}'\n",
    "table.append(temp)\n",
    "temp = '\\\\textsc{Method}'\n",
    "for task in tasks[dataset]:\n",
    "    temp += f' & {task}'\n",
    "temp += ' & Avg\\\\\\\\'\n",
    "table.append(temp)\n",
    "table.append('\\\\midrule')\n",
    "for ix, method in enumerate(methods):\n",
    "    if ix == 0:\n",
    "        pass\n",
    "    else:\n",
    "        table.append('\\\\midrule\\\\midrule')\n",
    "    if method in ['safn', 'ar']:\n",
    "        temp = '\\\\textsc{'+method+'}$^{\\\\dagger *}$'\n",
    "    else:\n",
    "        temp = '\\\\textsc{'+method+'}$^\\\\dagger$'\n",
    "    if method == 'ar':\n",
    "        mean = reported[dataset]['ar_linear']\n",
    "    else:\n",
    "        if method in reported[dataset].keys():\n",
    "            mean = reported[dataset][method]\n",
    "        else:\n",
    "            mean = np.zeros(len(reported[dataset]['s. only']))\n",
    "    for m in mean:\n",
    "        temp += f' & {m:.2f}'\n",
    "    mean = np.mean(mean)\n",
    "    temp += f' & {np.mean(mean):.2f}'\n",
    "    temp += '\\\\\\\\'\n",
    "    table.append(temp)\n",
    "    if method == 'safn':\n",
    "        temp = '\\\\textsc{'+method+'}* (Ours)'\n",
    "        mean = tables_nonlinear[dataset]['oracle']['oracle'].mean(axis=2)[0]*100\n",
    "        stds = tables_nonlinear[dataset]['oracle']['oracle'].std(axis=2)[0]*100\n",
    "        for m,std in zip(mean,stds):\n",
    "            temp += f' & {m:.2f}'\n",
    "        mean = tables_nonlinear[dataset]['oracle']['oracle'].mean(axis=1)[0]*100\n",
    "        temp += f' & {np.mean(mean):.2f}'\n",
    "        temp += '\\\\\\\\'\n",
    "        table.append(temp)\n",
    "    elif method == 'ar':\n",
    "        temp = '\\\\textsc{'+method+'}* (Ours)'\n",
    "        mean = tables_radius[dataset]['oracle']['oracle'].mean(axis=2)[0]*100\n",
    "        stds = tables_radius[dataset]['oracle']['oracle'].std(axis=2)[0]*100\n",
    "        for m,std in zip(mean,stds):\n",
    "            temp += f' & {m:.2f}'\n",
    "        mean = tables_radius[dataset]['oracle']['oracle'].mean(axis=1)[0]*100\n",
    "        temp += f' & {np.mean(mean):.2f}'\n",
    "        temp += '\\\\\\\\'\n",
    "        table.append(temp)\n",
    "\n",
    "\n",
    "    temp = '\\\\textsc{'+method+'} (Ours)'\n",
    "    mean = tables[dataset]['oracle']['oracle'].mean(axis=2)[ix]*100\n",
    "    stds = tables[dataset]['oracle']['oracle'].std(axis=2)[ix]*100\n",
    "    for m,std in zip(mean,stds):\n",
    "        temp += f' & {m:.2f}'\n",
    "    mean = tables[dataset]['oracle']['oracle'].mean(axis=1)[ix]*100\n",
    "    temp += f' & {np.mean(mean):.2f}'\n",
    "    temp += '\\\\\\\\'\n",
    "    table.append(temp)\n",
    "\n",
    "table.append('\\\\end{tabular}}')\n",
    "table.append('\\\\caption{Comparison between reported ($\\\\dagger$) accuracies on partial \\\\textsc{office-home} from published methods with our implementation using the \\\\textsc{oracle} model selection strategy. * denotes different bottleneck architectures.}')\n",
    "table.append('\\\\label{table:office_home_comparison_reported}')\n",
    "table.append('\\\\end{table}')\n",
    "print('\\n'.join(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764cbef",
   "metadata": {},
   "source": [
    "## Code to generate Table 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'visda'\n",
    "table = ['\\\\begin{table}[t!]\\\\centering']\n",
    "temp = '\\\\begin{tabular}{c'\n",
    "for i in range(len(tasks)+1):\n",
    "    temp += '@{\\hskip 3pt}|c'\n",
    "temp += '}'\n",
    "table.append(temp)\n",
    "temp = '\\\\textsc{Algorithm}'\n",
    "for task in tasks[dataset]:\n",
    "    temp += f' & {task}'\n",
    "temp += ' & Avg\\\\\\\\'\n",
    "table.append(temp)\n",
    "table.append('\\\\midrule')\n",
    "for ix, method in enumerate(methods):\n",
    "    if method == 'gamma':\n",
    "        pass\n",
    "    else:\n",
    "        if ix == 0:\n",
    "            pass\n",
    "        else:\n",
    "            table.append('\\\\midrule\\\\midrule')\n",
    "        temp = '\\\\textsc{'+method+'}$^\\\\dagger$'\n",
    "        if method == 'ar':\n",
    "            temp = '\\\\textsc{'+method+'}$^{\\\\dagger *}$'\n",
    "            mean = reported[dataset]['ar_linear']\n",
    "        else:\n",
    "            if method in reported[dataset].keys():\n",
    "                mean = reported[dataset][method]\n",
    "            else:\n",
    "                mean = np.zeros(len(reported[dataset]['s. only']))\n",
    "        mean_task = None\n",
    "        for m in mean:\n",
    "            if isinstance(m,str) or m == 0.0:\n",
    "                temp += ' & -'\n",
    "                mean_task = 'na'\n",
    "            else:\n",
    "                temp += f' & {m:.2f}'\n",
    "        if mean_task == 'na':\n",
    "            temp += ' & -'\n",
    "        else:\n",
    "            temp += f' & {np.mean(mean):.2f}'\n",
    "        temp += '\\\\\\\\'\n",
    "        table.append(temp)\n",
    "        temp = '\\\\textsc{'+method+'} (Ours)'\n",
    "        mean = tables[dataset]['oracle']['oracle'].mean(axis=2)[ix]*100\n",
    "        stds = tables[dataset]['oracle']['oracle'].std(axis=2)[ix]*100\n",
    "        for m,std in zip(mean,stds):\n",
    "            temp += f' & {m:.2f}'\n",
    "        mean = tables[dataset]['oracle']['oracle'].mean(axis=1)[ix]*100\n",
    "        temp += f' & {np.mean(mean):.2f}'\n",
    "        temp += '\\\\\\\\'\n",
    "        table.append(temp)\n",
    "\n",
    "\n",
    "table.append('\\\\end{tabular}')\n",
    "table.append('\\\\caption{Comparison between reported ($\\\\dagger$) accuracies on partial \\\\textsc{visda} from published methods with our implementation using the \\\\textsc{oracle} model selection strategy. * denotes different bottleneck architectures.}')\n",
    "table.append('\\\\label{table:visda_comparison_reported}')\n",
    "table.append('\\\\end{table}')\n",
    "print('\\n'.join(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd128b",
   "metadata": {},
   "source": [
    "## Code to generate Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_metrics = {'s_acc': 's-acc', 'ent': 'ent', 'dev_svm': 'dev', 'snd': 'snd', '1shot_acc': '1-shot',\n",
    "              '100random_acc': '100-rnd', 'oracle': 'oracle'}\n",
    "table = ['\\\\begin{table}[t!]\\\\centering']\n",
    "table.append('\\\\resizebox{\\\\textwidth}{!}{')\n",
    "temp = '\\\\begin{tabular}{c|@{\\hskip 3pt}c|'\n",
    "for method in methods:\n",
    "    temp += '@{\\hskip 3pt}|@{\\hskip 3pt}c'\n",
    "temp += '}'\n",
    "table.append(temp)\n",
    "\n",
    "temp = '\\\\textsc{Dataset} & Model Selection'\n",
    "for method in methods:\n",
    "    temp += ' & \\\\textsc{'+method+'}'\n",
    "temp += '\\\\\\\\'\n",
    "table.append(temp)\n",
    "\n",
    "for dataset in ['office-home', 'visda']:\n",
    "    table.append('\\\\midrule\\\\midrule')\n",
    "    aux_table = np.zeros((len(methods), len(hp_metrics)))\n",
    "    aux_table_stds = np.zeros((len(methods), len(hp_metrics)))\n",
    "    for ix, method in enumerate(methods):\n",
    "        for jx, metric in enumerate(hp_metrics):\n",
    "            aux_table[ix,jx] = tables[dataset][metric][metric].mean(axis=1).mean(axis=1)[ix]*100\n",
    "            aux_table_stds[ix,jx] = tables[dataset][metric][metric].mean(axis=1).std(axis=1)[ix]*100\n",
    "    new_table = np.zeros((len(methods),3))\n",
    "    new_table_stds = np.zeros((len(methods),3))\n",
    "    new_table[:,0] = aux_table[:,:4].min(axis=1)\n",
    "    new_table_stds[:,0] = aux_table_stds[np.arange(len(methods)),aux_table[:,:4].argmin(axis=1)]\n",
    "    new_table[:,1] = aux_table[:,:4].max(axis=1)\n",
    "    new_table_stds[:,1] = aux_table_stds[np.arange(len(methods)),aux_table[:,:4].argmax(axis=1)]\n",
    "    new_table[:,2] = aux_table[:,-1]\n",
    "    new_table_stds[:,2] = aux_table_stds[:,-1]\n",
    "    new_table = new_table.transpose()\n",
    "    new_table_stds = new_table_stds.transpose()\n",
    "    temp = '\\\\multirow{2}{*}{\\\\textsc{'+dataset+'}} & Worst (w/o target labels)'\n",
    "    worse_selection = ['(\\\\textsc{'+list(hp_metrics.values())[i]+'})' for i in aux_table[:,:4].argmin(axis=1)]\n",
    "    for i in range(len(methods)):\n",
    "        temp += f' & {new_table[0,i]:.2f} ({new_table[0,i]-new_table[2,i]:.2f})'\n",
    "    temp += '\\\\\\\\'\n",
    "    table.append(temp)\n",
    "    temp = ' & Best (w/o target labels)'\n",
    "    best_selection = ['(\\\\textsc{'+list(hp_metrics.values())[i]+'})' for i in aux_table[:,:4].argmax(axis=1)]\n",
    "    for i in range(len(methods)):\n",
    "        temp += f' & {new_table[1,i]:.2f} ({new_table[1,i]-new_table[2,i]:.2f})'\n",
    "    temp += '\\\\\\\\'\n",
    "    table.append(temp)\n",
    "    temp = ' & \\\\textsc{{oracle}}'\n",
    "    for i in range(len(methods)):\n",
    "        temp += f' & {new_table[2,i]:.2f}'\n",
    "    temp += '\\\\\\\\'\n",
    "    table.append(temp)\n",
    "table.append('\\\\end{tabular}}')\n",
    "table.append('\\\\caption{Task accuracy average computed over three different seeds (2020, 2021, 2022) on Partial \\\\textsc{office-home} and Partial-\\\\textsc{visda}. For each dataset and PDA method, we display the results of the worst and best performing model selection that do not use target labels as well as the \\\\textsc{oracle} model selection strategy. All results can be found in Table \\\\ref{table:overall_results}.}')\n",
    "table.append('\\\\label{table:small_overall_results}')\n",
    "table.append('\\\\end{table}')\n",
    "print('\\n'.join(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d1bee6",
   "metadata": {},
   "source": [
    "## Code to generate Table 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185a6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = ['\\\\begin{table}[t!]\\\\centering']\n",
    "table.append('\\\\resizebox{\\\\textwidth}{!}{')\n",
    "temp = '\\\\begin{tabular}{c|@{\\hskip 3pt}c|'\n",
    "for metric in hp_metrics:\n",
    "    temp += '@{\\hskip 3pt}|c'\n",
    "temp += '}'\n",
    "table.append(temp)\n",
    "temp = '\\\\textsc{Dataset} & \\\\textsc{Method}'\n",
    "for metric in hp_metrics:\n",
    "    temp += ' & \\\\textsc{'+hp_metrics[metric]+'}'\n",
    "temp += '\\\\\\\\'\n",
    "table.append(temp)\n",
    "for kx, dataset in enumerate(['office-home', 'visda']):\n",
    "    table.append('\\\\midrule\\\\midrule')\n",
    "    aux_table = np.zeros((len(methods), len(hp_metrics)))\n",
    "    for ix, method in enumerate(methods):\n",
    "        for jx, metric in enumerate(hp_metrics):\n",
    "            aux_table[ix,jx] = tables[dataset][metric][metric].mean(axis=1).mean(axis=1)[ix]*100\n",
    "    max_ix = aux_table.argmax(axis=0)\n",
    "    for ix, method in enumerate(methods):\n",
    "        if ix == 0:\n",
    "            temp = '\\\\multirow{7}{*}{\\\\textsc{'+dataset+'}} & \\\\textsc{'+method+'} '\n",
    "        else:\n",
    "            temp = ' & \\\\textsc{'+method+'} '\n",
    "        for jx, metric in enumerate(hp_metrics):\n",
    "            mean = tables[dataset][metric][metric].mean(axis=1).mean(axis=1)[ix]*100\n",
    "            std = tables[dataset][metric][metric].mean(axis=1).std(axis=1)[ix]*100\n",
    "#             temp += f'& {mean:.2f}$\\\\:\\\\pm\\\\:${std:.1f}'\n",
    "            if ix == max_ix[jx]:\n",
    "                temp += '& \\\\textbf{'+f'{mean:.2f}$\\\\pm${std:.1f}'+'}'\n",
    "            else:\n",
    "                temp += f'& {mean:.2f}$\\\\pm${std:.1f}'\n",
    "        temp += '\\\\\\\\'\n",
    "        table.append(temp)    \n",
    "table.append('\\\\end{tabular}}')\n",
    "table.append('\\\\caption{Task accuracy average for the different PDA methods and model selection strategy pairs on Partial Office-Home and Partial VisDA. The average is computed over three difference seeds (2020, 2021, 2022).}')\n",
    "table.append('\\\\label{table:overall_results}')\n",
    "table.append('\\\\end{table}')\n",
    "print('\\n'.join(table))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8bf232",
   "metadata": {},
   "source": [
    "## Code to generate Table 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3ccd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = 'office-home'\n",
    "table = ['\\\\begin{table}[t!]\\\\centering']\n",
    "if dataset == 'office-home':\n",
    "    table.append('\\\\resizebox{\\\\textwidth}{!}{')\n",
    "temp = '\\\\begin{tabular}{c'\n",
    "for i in range(len(tasks[dataset])+2):\n",
    "    temp += '@{\\hskip 3pt}|{\\hskip 3pt}c'\n",
    "temp += '}'\n",
    "table.append(temp)\n",
    "temp = '\\\\textsc{Metric} & \\\\textsc{Method}'\n",
    "for task in tasks[dataset]:\n",
    "    temp += f' & {task}'\n",
    "temp += ' & Avg\\\\\\\\'\n",
    "table.append(temp)\n",
    "\n",
    "for metric in hp_metrics:\n",
    "    table.append('\\\\midrule\\\\midrule')\n",
    "\n",
    "    aux_table = np.zeros((len(methods), len(tasks[dataset])+1))\n",
    "    for ix, method in enumerate(methods):\n",
    "        aux_table[ix,:-1] = tables[dataset][metric][metric].mean(axis=2)[ix]*100\n",
    "    aux_table[:,-1] = aux_table[:,:-1].mean(axis=1)\n",
    "    max_ix = aux_table.argmax(axis=0)\n",
    "    for ix, method in enumerate(methods):\n",
    "        if ix == 0:\n",
    "            temp = '\\\\multirow{'+str(len(methods))+'}{*}{\\\\textsc{'+hp_metrics[metric]+'}}'\n",
    "        else:\n",
    "            temp = ''\n",
    "        temp += ' & \\\\textsc{'+method+'}'\n",
    "        mean = tables[dataset][metric][metric].mean(axis=2)[ix]*100\n",
    "        stds = tables[dataset][metric][metric].std(axis=2)[ix]*100\n",
    "        jx = 0\n",
    "        for m,std in zip(mean,stds):\n",
    "            if ix == max_ix[jx]:\n",
    "                temp += ' & \\\\textbf{'+f'{m:.2f}$\\\\:\\\\pm\\\\:${std:.1f}'+'}'\n",
    "            else:\n",
    "                temp += f' & {m:.2f}$\\\\:\\\\pm\\\\:${std:.1f}'\n",
    "            jx += 1\n",
    "\n",
    "        mean = tables[dataset][metric][metric].mean(axis=1)[ix]*100\n",
    "        if ix == max_ix[jx]:\n",
    "            temp += ' & \\\\textbf{'+f'{np.mean(mean):.2f}$\\\\:\\\\pm\\\\:${np.std(mean):.1f}'+'}'\n",
    "        else:\n",
    "            temp += f' & {np.mean(mean):.2f}$\\\\:\\\\pm\\\\:${np.std(mean):.1f}'\n",
    "        temp += '\\\\\\\\'\n",
    "        table.append(temp)\n",
    "table.append('\\\\end{tabular}}')\n",
    "table.append('\\\\caption{Average accuracy of different PDA methods based on different model selection strategies on the 12 tasks of Partial \\\\textsc{office-home}. Average is done over three seeds (2020, 2021, 2022). Best results in \\\\textbf{bold}.}')\n",
    "table.append('\\\\label{tab:office_home_results_tasks_with_stds}')\n",
    "table.append('\\\\end{table}')\n",
    "print('\\n'.join(table))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa680d3",
   "metadata": {},
   "source": [
    "## Code to generate Table 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16398b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'visda'\n",
    "table = ['\\\\begin{table}[t!]\\\\centering']\n",
    "table.append('\\\\resizebox{\\\\textwidth}{!}{')\n",
    "if dataset == 'office-home':\n",
    "    table.append('\\\\resizebox{\\\\textwidth}{!}{')\n",
    "temp = '\\\\begin{tabular}{c'\n",
    "for i in range(len(hp_metrics)+1):\n",
    "    temp += '@{\\hskip 3pt}|c'\n",
    "temp += '}'\n",
    "table.append(temp)\n",
    "temp = '\\\\textsc{Task} & \\\\textsc{Method}'\n",
    "for metric in hp_metrics:\n",
    "    temp += f' & '+'\\\\textsc{'+hp_metrics[metric]+'}'\n",
    "temp += '\\\\\\\\'\n",
    "table.append(temp)\n",
    "\n",
    "for jx, task in enumerate(tasks[dataset]):\n",
    "    table.append('\\\\midrule\\\\midrule')\n",
    "\n",
    "    aux_table = np.zeros((len(methods),len(hp_metrics)))\n",
    "    for i, method in enumerate(methods):\n",
    "        for j, metric in enumerate(hp_metrics):\n",
    "            aux_table[i,j] = tables[dataset][metric][metric][:,jx,:].mean(axis=1)[i]*100\n",
    "    max_ix = aux_table.argmax(axis=0)\n",
    "\n",
    "    for ix, method in enumerate(methods):\n",
    "        if ix == 0:\n",
    "            temp = '\\\\multirow{'+str(len(methods))+'}{*}{'+task+'}'\n",
    "        else:\n",
    "            temp = ''\n",
    "        temp += ' & \\\\textsc{'+method+'}'\n",
    "        for j, metric in enumerate(hp_metrics):\n",
    "            mean = tables[dataset][metric][metric][:,jx,:].mean(axis=1)[ix]*100\n",
    "            std = tables[dataset][metric][metric][:,jx,:].std(axis=1)[ix]*100\n",
    "            if ix == max_ix[j]:\n",
    "                temp += ' & \\\\textbf{'+f'{mean:.2f}$\\\\:\\\\pm\\\\:${std:.1f}'+'}'\n",
    "            else:\n",
    "                temp += f' & {mean:.2f}$\\\\:\\\\pm\\\\:${std:.1f}'\n",
    "        temp += '\\\\\\\\'\n",
    "        table.append(temp)\n",
    "table.append('\\\\midrule\\\\midrule')\n",
    "\n",
    "aux_table = np.zeros((len(methods),len(hp_metrics)))\n",
    "for i, method in enumerate(methods):\n",
    "    for j, metric in enumerate(hp_metrics):\n",
    "        aux_table[i,j] = tables[dataset][metric][metric].mean(axis=1).mean(axis=1)[i]*100\n",
    "max_ix = aux_table.argmax(axis=0)\n",
    "for ix, method in enumerate(methods):\n",
    "    if ix == 0:\n",
    "        temp = '\\\\multirow{'+str(len(methods))+'}{*}{Avg}'\n",
    "    else:\n",
    "        temp = ''\n",
    "    temp += ' & \\\\textsc{'+method+'}'\n",
    "    for j, metric in enumerate(hp_metrics):\n",
    "        mean = tables[dataset][metric][metric].mean(axis=1).mean(axis=1)[ix]*100\n",
    "        std = tables[dataset][metric][metric].mean(axis=1).std(axis=1)[ix]*100\n",
    "        if ix == max_ix[j]:\n",
    "            temp += ' & \\\\textbf{'+f'{mean:.2f}$\\\\:\\\\pm\\\\:${std:.1f}'+'}'\n",
    "        else:\n",
    "            temp += f' & {mean:.2f}$\\\\:\\\\pm\\\\:${std:.1f}'\n",
    "    temp += '\\\\\\\\'\n",
    "    table.append(temp)\n",
    "table.append('\\\\end{tabular}}')\n",
    "table.append('\\\\caption{Accuracy of different PDA methods based on different model selection strategies on the 2 Partial \\\\textsc{visda} tasks. Average is done over three seeds (2020, 2021, 2022). Best results in \\\\textbf{bold}.}')\n",
    "table.append('\\\\label{table:visda_results_tasks}')\n",
    "table.append('\\\\end{table}')\n",
    "print('\\n'.join(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b5d29",
   "metadata": {},
   "source": [
    "## Code to generate Table 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_metrics = {'s_acc': 's-acc', 'ent': 'ent', 'dev_svm': 'dev', 'snd': 'snd', '1shot_acc': '1-shot',\n",
    "              '50random_acc': '50-rnd', '100random_acc': '100-rnd', 'oracle': 'oracle'}\n",
    "table = ['\\\\begin{table}[t!]\\\\centering']\n",
    "table.append('\\\\resizebox{\\\\textwidth}{!}{')\n",
    "temp = '\\\\begin{tabular}{c|@{\\hskip 3pt}c|'\n",
    "for metric in hp_metrics:\n",
    "    temp += '@{\\hskip 3pt}|c'\n",
    "temp += '}'\n",
    "table.append(temp)\n",
    "temp = '\\\\textsc{Dataset} & \\\\textsc{Method}'\n",
    "for metric in hp_metrics:\n",
    "    temp += ' & \\\\textsc{'+hp_metrics[metric]+'}'\n",
    "temp += '\\\\\\\\'\n",
    "table.append(temp)\n",
    "for kx, dataset in enumerate(['office-home', 'visda']):\n",
    "    table.append('\\\\midrule\\\\midrule')\n",
    "    aux_table = np.zeros((len(methods), len(hp_metrics)))\n",
    "    for ix, method in enumerate(methods):\n",
    "        for jx, metric in enumerate(hp_metrics):\n",
    "            aux_table[ix,jx] = tables[dataset][metric][metric].mean(axis=1).mean(axis=1)[ix]*100\n",
    "    max_ix = aux_table.argmax(axis=0)\n",
    "    for ix, method in enumerate(methods):\n",
    "        if ix == 0:\n",
    "            temp = '\\\\multirow{7}{*}{\\\\textsc{'+dataset+'}} & \\\\textsc{'+method+'} '\n",
    "        else:\n",
    "            temp = ' & \\\\textsc{'+method+'} '\n",
    "        for jx, metric in enumerate(hp_metrics):\n",
    "            mean = tables[dataset][metric][metric].mean(axis=1).mean(axis=1)[ix]*100\n",
    "            std = tables[dataset][metric][metric].mean(axis=1).std(axis=1)[ix]*100\n",
    "#             temp += f'& {mean:.2f}$\\\\:\\\\pm\\\\:${std:.1f}'\n",
    "            if ix == max_ix[jx]:\n",
    "                temp += '& \\\\textbf{'+f'{mean:.2f}$\\\\pm${std:.1f}'+'}'\n",
    "            else:\n",
    "                temp += f'& {mean:.2f}$\\\\pm${std:.1f}'\n",
    "        temp += '\\\\\\\\'\n",
    "        table.append(temp)    \n",
    "table.append('\\\\end{tabular}}')\n",
    "table.append('\\\\caption{Task accuracy average for the different PDA methods and model selection strategy pairs on Partial Office-Home and Partial VisDA. The average is computed over three difference seeds (2020, 2021, 2022).}')\n",
    "table.append('\\\\label{table:overall_results}')\n",
    "table.append('\\\\end{table}')\n",
    "print('\\n'.join(table))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b41856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70adc339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450b439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad718c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
